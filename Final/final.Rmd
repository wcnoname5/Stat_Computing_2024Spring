---
title: "Statistical Computing Final Project"
author: "Wei-Chen Chang r12227118"
date: "Due: 2024-06-05"
output:
  pdf_document:
    fig_caption: yes
    extra_dependencies: amsmath
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width= 5.6,fig.height=4.0, fig.align = "center")
```

```{r install nimble, eval=FALSE, include=FALSE}
install.packages("nimble")
```

# The Model

Suppose $Y_{ij}$ are the birthweight for gender $i$ , and assume 
$$
Y_{ij}\mathop{\sim}\limits^{ind} N(\mu_i,\sigma^2),\; \mu_i\mathop{\sim}\limits^{ind} N(\eta,\tau^2)
$$
for $i=1,2,j=1,...,n_i;n=n_1+n_2$, and prior

$$
\pi(\eta,\tau^2,\sigma^2)\propto IG(\tau^2; a_\tau,b_\tau)\cdot Ca^+(\sigma^2;0,b_\sigma)
$$


## Load dataset
Load dataset, take a look, and setting variables.
```{r data, message=FALSE}
library(lattice)
library(tidyverse)
# Load Dataset
data(birthweight,package="LearnBayes")
bweight.list <- list("gender" = birthweight$gender+1,
                     "weight"= birthweight$weight,
                     "N" = nrow(birthweight),
                     "eta_mean" = mean(birthweight$weight))

mygender <- with(birthweight, ifelse(birthweight$gender==0,"Male","Female"))
lattice::dotplot(mygender ~ weight, data = birthweight,
xlab = "Birthweight", ylab = "Gender")

bw_summerize <-  birthweight|>
  group_by(gender)|>
  summarise(N=n(),
            weight.mean = mean(weight),
            weight.sd = sd(weight),
            weight.var = var(weight))
n.gender <- bw_summerize$N
ybar <- bw_summerize$weight.mean
bw_summerize
```

```{r eval=FALSE, include=FALSE}
t.test(weight~gender, data = birthweight)
```

Moreover, here compute the variance between the sample mean different gender, which is `r round(var(bw_summerize$weight.mean),2)`.

# 1.  Implementation in JAGS

## Specifying Model in JAGS
Since there's no Cauchy distribution function in JAGS, we use $t(x;\mu=0,\tau,k=1)$ via `dt` Cauchy distribution $Ca(x;0,b)$. Note that in such parameterization in JAGS, $\tau = \frac{1}{b^2}$.

The hyperparameters $b_\tau, b_\sigma$ in Cauchy priors of $\tau^2, \sigma^2$ 
were set as 10,000 and 100,000 because the sample variance from the data is large.
Moreover, here we let $\eta$ follows a normal hyperprior with 
$\mu_\eta= \frac{1}{(n_1+n_2)}\sum_{i=1}^2\sum_{j=1}^{n_i}{y_i}$, $\sigma_\eta^2=10000$.

Lastly, the intial values for 5 parameters were generated by the distribution below:
$$
\begin{aligned}
\mu_i, \eta &\sim N(3000, \sigma^2 = 500^2)\\
\tau^2      &\sim gamma(a=10, b=50)\\
\sigma^2    &\sim gamma(a=10, b=500)
\end{aligned}
$$

```{r model in JAGS}
# JAGS model
modelstring ="
model{
  for(subID in 1:N){
    weight[subID] ~ dnorm(mu[gender[subID]], preci[1])
  }
  for (i in 1:2){
    mu[i] ~ dnorm(eta, preci[2])
  }
  preci[1] <- 1/sigma.sq
  preci[2] <- 1/tau.sq
  
## priors ##
  eta ~ dnorm(eta_mean, 1/10000)
  tau.sq ~ dt(0, t.tau_tau.sq, 1) T(0,)
  t.tau_tau.sq <- 1/b.tau^2
  
  sigma.sq ~ dt(0, t.tau_sigma.sq, 1) T(0,) 
  t.tau_sigma.sq <- 1/b.sigma^2
  b.tau <-  10000
  b.sigma <- 100000
}
"
writeLines(modelstring, con="bweight.bug")

```

```{r eval=FALSE, include=FALSE}
curve(dgamma(x, shape=50, scale = 1000), from=0, to =100000)
curve(dinvgamma(x, shape=1, scale = 10000), from=0, to =10000)
curve((\(x) 2*dcauchy(x,scale = 10))(x), from=0, to = 10)
```

### Run jags model:
```{r message=FALSE}
library(coda)
library(rjags)
library(R2jags)
param.names <- c("mu", "eta", "tau.sq","sigma.sq")
# set initial values

set.seed(99)
bayes.mod.inits <- function(tau.scale=50, sigma.scale=500){
 list("mu" = rnorm(2, 3000, 500),
      "eta" = rnorm(1, 3000, 500),
      "tau.sq" = rgamma(1, 10, scale=tau.scale),
      "sigma.sq" = rgamma(1, 10, scale=sigma.scale)
      )
}
inits <- list(bayes.mod.inits(), bayes.mod.inits(), bayes.mod.inits())
bweight.jags<- jags(data = bweight.list,
     inits = inits,
     parameters.to.save = param.names,
     n.chains=3, n.iter=10000, n.burnin=1000,
     model.file = "bweight.bug")

bweight.jags |> print()
```
For visulaizing purpose, the posterior density plot and other diagnositic plots were shown below.

```{r JAGS_diag}
jags.mcmc <- as.mcmc(bweight.jags)
gelman.diag(jags.mcmc)
densityplot(jags.mcmc, layout=c(3,2), aspect="fill", 
       main = "Density Plot of JAGS")
xyplot(jags.mcmc, layout=c(3,2), aspect="fill", 
       main = "Divergence Plot of JAGS")
acfplot(jags.mcmc, main = "ACF Plot of JAGS", layout=c(3,2), aspect="fill")
```

## Comments

All parameters seems converge ok from the density plot and caterpillar plot,
but $\tau^2$ seem suffer from autocorrelation between samples (See ACF plot).

The difference between posterior mean of $\mu_1, \mu_2$ smaller than difference of sample mean. And $\sigma^2$ and $\tau^2$ is much larger than sample estimates.


# 2-4.

To make life easier, A Gibbs sampler function were written to implement different
sampling methods for generating $\sigma$. 

## Gibbs Sampler functions

To achieve such goal, first all other sampler from full conditional distribution 
must be specified first.

To align the result from JAGS, for the following MCMC sequences, there are 10000 iterations 
for each, first 1000 samples were burnt-in, and thinning 9 samples, Resulting a 1000 valid samples 
for each chain. And each sampling method runs 3 chains.

Lastly, the value of hyperparameters were assigned as below: $a_\tau = 1, b_\tau = 10000$ 
in Inverse-Gamma priors, and $b_\sigma = 10000.$ in truncated Cauchy priors

```{r full conditional functions, message=FALSE}
library(nimble) # help generate inverted gamma
# ---- MCMC Generators ----
mu_gen <- function(i, eta, sigma.sq, tau.sq){
  if (!(i %in% 1:2)){
    return("invalid i")}
  Nor.mu <- (1/(sigma.sq/n.gender[i]) + 1/tau.sq)^(-1) * (ybar[i]/(sigma.sq/n.gender[i]) + eta/tau.sq)
  Nor.sig_sq <- (1/(sigma.sq/n.gender[i]) + 1/tau.sq)^(-1)
  rnorm(1, Nor.mu, sqrt(Nor.sig_sq))
}

eta_gen <- function(mu, # 2*1 vector
                          tau.sq) {
  mu_bar <-  mean(mu)
  rnorm(1, mu_bar, sqrt(tau.sq/2))
}

tau.sq_gen <- function(mu, # 2*1 vector
                       eta, a.tau, b.tau){
  shape <-  a.tau+1
  scale <-  mean((mu-eta)^2) + b.tau
  rinvgamma(1, shape = shape, scale = scale) # from `nimble`
}

sigma.sq_gen <- function(mu, b.sigma, last_sigma.sq,
    type=c("AR", "MH.ind","MH"), y=birthweight){
  type <- match.arg(type)
  a <- (nrow(y)/2)-1
  b <- ((sum(y[y$gender==0,"weight"] - mu[1])^2) + (sum(y[y$gender==1,"weight"] - mu[2])^2))/2
  Accept <- FALSE
  while (!Accept) {
    sigma.sq <- ifelse(type == "MH",
                       rchisq(1, df = last_sigma.sq),
                       rinvgamma(1, shape = a, scale = b))

    if (type == "AR"){ #A-R sampling
      c_max <- dcauchy(0, location = 0, scale = b.sigma)
      accept_rate <- 
        dcauchy(sigma.sq, 0, b.sigma)/c_max 
      # truncaction constant were cancel out
    }
    else if (type == "MH.ind"){ #MH independent
      prop <- dcauchy(sigma.sq, 0, b.sigma)/dcauchy(last_sigma.sq, 0, b.sigma)
      accept_rate <- min(1, prop)
    }
    else if(type == "MH"){
      q_func <- function(x){
        dinvgamma(x, shape = a, scale = b)*2*dcauchy(x, scale = b.sigma)*(x>0)
      }
      num <- q_func(sigma.sq) * dchisq(last_sigma.sq, df = sigma.sq)
      deno <- q_func(last_sigma.sq) * dchisq(sigma.sq, df = last_sigma.sq)
      accept_rate <- ifelse(deno==0, 1 , min((num/deno), 1))
    }
    u <- runif(1)
    Accept <- (u < accept_rate)
  } 
  return(sigma.sq)
}

# ---- The Gibbs-Sampler function ----
gibbs <- function(inits, Nsim, type = c("AR", "MH.ind","MH"),
                  hyp.param=list(a.tau=1,b.tau=10000,b.sigma=10000),
                  burnin=1000, thining=9, chain=3){
  type <- match.arg(type)
  MCMC_chain <- function(init){
    MCMC.mtx <- 
      matrix(nrow = Nsim, ncol = 5,
             dimnames = 
               list(c(), c("mu1", "mu2", "eta","sigma.sq","tau.sq"))
             )
    mu1 <- init$mu[1]
    mu2 <- init$mu[2]
    eta <- init$eta
    sigma.sq <- init$sigma.sq
    tau.sq <- init$tau.sq
    MCMC.mtx[1, ] <- c(mu1, mu2, eta, sigma.sq, tau.sq)
    # hyperparameters
    a.tau <- hyp.param$a.tau # larger a, Expectation smaller
    b.tau <- hyp.param$b.tau
    b.sigma <- hyp.param$b.sigma
    for (iter in 2:Nsim){
      mu1 <- mu_gen(1, eta, sigma.sq, tau.sq)
      mu2 <- mu_gen(2, eta, sigma.sq, tau.sq)
      eta <- eta_gen(mu=c(mu1, mu2), tau.sq)
      tau.sq <- tau.sq_gen(mu = c(mu1, mu2), eta = eta,
                           a.tau = a.tau, b.tau = b.tau)
      sigma.sq <- sigma.sq_gen(mu = c(mu1, mu2), b.sigma=b.sigma,
                               last_sigma.sq = sigma.sq, type=type)
      # print(MCMC.mtx)
      MCMC.mtx[iter, ] <- c(mu1, mu2, eta, sigma.sq, tau.sq)
    }
    MCMC.mtx <- MCMC.mtx[seq(1+burnin, Nsim, by=thining),]
    mcmc(MCMC.mtx, start = burnin+thining, end = Nsim, thin = thining)
  }
  for (i in 1:chain){
    if (i==1){
      MCMC_list <- list(MCMC_chain(inits[[i]]))
    }
    else{
      MCMC_list[[i]] <- MCMC_chain(inits[[i]])
    }
  } 
  mcmc.list(MCMC_list)
  # return(mcmc.list(MCMC_list))
}
```

# 2. AR method
Acceptance-Reject sampling with $\sigma^* \sim IG(a, b)$ and thus $C_{max} = Ca^+(0; 0, b_\sigma)$ and the
acceptance probability is $Ca^+(\sigma^*; 0, b_\sigma)/C_{max}$.

Note that $a = \frac{n}{2}-1$, $b=\sum_{i=2}^{2}\sum_{j=1}^{n_i}(y_{ij}-\mu_i)^2/2.$

Here the starting values were set as following:
```{r AR, warning=FALSE}
set.seed(99)
inits <- list(bayes.mod.inits(tau.scale=50), bayes.mod.inits(tau.scale=50), bayes.mod.inits(tau.scale=50))
AR.mcmc <- gibbs(inits, Nsim=10000,
                 burnin=1000, thining=9, chain=3,
                 type = "AR")#,chain=1)
# resluts
summary(AR.mcmc)
gelman.diag(AR.mcmc)
densityplot(AR.mcmc, layout=c(3,2), aspect="fill", 
       main = "Density Plot of AR")
xyplot(AR.mcmc, layout=c(3,2), aspect="fill", 
       main = "Divergence Plot of AR")
acfplot(AR.mcmc, main = "ACF Plot of AR", layout=c(3,2), aspect="fill")
```
## Comments

The convergence of parameters seems to be problematic in Rubin-Gelman statistics (Potential scale reduction factors), especially for $\mu_1$. But From the density plot and the caterpillar plot one can find the variation of estimates were surprisingly very small. The empirical s.d. of $\mu_1$, $\mu_2$ is even about $10^{-12}$!

The posterior mean of $\mu_1,\mu_2$ is identical to sample estimates. But $\sigma^2$
is extremely close to 0 (about $10^{-20}$), which is unreasonable compare to the 
real data. The value of $\tau^2$ is quite reasonable, though.



# 3. independence M-H sampling 
Independence Metropolis-Hastings with $\sigma^* \sim IG(a, b)$ 
and thus the acceptance probability is
$$\text{min}\{Ca^+(\sigma^*; 0, b_\sigma)/Ca^+(\sigma^{(t)}; 0, b_\sigma), 1\}$$.
```{r MH_ind, warning=FALSE}
set.seed(99)
inits <- list(bayes.mod.inits(), bayes.mod.inits(), bayes.mod.inits())
MH.ind.mcmc <- gibbs(inits,
            Nsim=10000, burnin=1000, thining=9, chain=3,
            type = "MH.ind")

summary(MH.ind.mcmc)
gelman.diag(MH.ind.mcmc)
densityplot(MH.ind.mcmc, layout=c(3,2), aspect="fill", 
       main = "Density Plot of MH(indep.)")
xyplot(MH.ind.mcmc, layout=c(3,2), aspect="fill", 
       main = "Divergence Plot of MH(indep.)")
acfplot(MH.ind.mcmc, main = "ACF Plot of MH(indep.)", layout=c(3,2), aspect="fill")

```
## Comments

The results of estimation is similar to AR method. Though the G-R statistic looks better it just
the magic of seeds.

Notice that the ACF of M-H (independence sampling) is more volatile than AR.
It's reasonable because the MH algorithm follows Markov-Chain. Moreover, MH seems to be faster and more tolerable to different initial values.


# 4. Metroplis-Hastings with $\sigma^* \sim \chi^2(\cdot|\sigma^{(t)})$
The acceptance probability is
$$
\text{min}\{\frac{q(\sigma^*)\cdot\chi^2(\sigma^{(t)}|\sigma^*)}
{q(\sigma^{(t)})\cdot\chi^2(\sigma^*|\sigma^{(t)})},
1\},
$$
where $q(\sigma) = IG(\sigma^2; a, b) · Ca+(\sigma^2; 0, b_\sigma).$

Sampling heavily relys on the value of hyperparameters. To make the sampler works,
the hyperparameters in current Metroplis-Hastings algorithm 
were changed as below: $a_\tau = 1, b_\tau = 100$ 
in Inverse-Gamma priors, and $b_\sigma = 1000$ in truncated Cauchy priors.
```{r MH_MCMC}
set.seed(99)
inits <- list(bayes.mod.inits(), bayes.mod.inits(), bayes.mod.inits())
hyperparams <- list(a.tau =1, b.tau = 100, b.sigma=1000)
MH.mcmc <- gibbs(inits, hyp.param = hyperparams,
            Nsim = 10000, burnin=1000, thining=9, chain=3,
            type = "MH")
## results
summary(MH.mcmc)
gelman.diag(MH.mcmc)
densityplot(MH.mcmc, layout=c(3,2), aspect="fill", 
       main = "Density Plot of MH")
xyplot(MH.mcmc, layout=c(3,2), aspect="fill", 
       main = "Divergence Plot of MH")
acfplot(MH.mcmc, main = "ACF Plot of MH", layout=c(3,2), aspect="fill")

```
## Comments

$\sigma^2$ is not converged, and suffers from autocorrelation between samples.
One can see clearly from caterpillar plot. Maybe more sampling could fix this problem.
Overall, the estimates(posterior) were alike to JAGS, but with much smaller $\tau^2$ and also difference between $\mu_1$, $\mu_2$.

```{r abandon, eval=FALSE, include=FALSE}
plot_mcmc <- function(mcmc.data, type = c("density", "line"), param = c("mu1", "mu2", "eta", "sigma.sq", "tau.sq"), chain=3){
  type <- match.arg(type)
  par(mfrow = c(2,3))
  for (p in param){
    for (i in 1:chain){
      if (i ==1){
        if (type=="density"){
          plot(density(mcmc.data[[1]][ ,p]),
               main =paste("density plot of", p))
        }else{
          plot(mcmc.data[[1]][ ,p], type="l",ylab = p,
               main =paste("caterpillar plot of", p))
        }
        
      }else{
        if(type=="density"){
          lines(density(mcmc.data[[i]][ ,p]), col=i)
        }else{
          lines(mcmc.data[[i]][ ,p], type="l", col=i)
        }
      }
    }
  }
}
plot_mcmc(rs, type= "line")


array(dim=)
matrix(nrow = Nsim, ncol = 5,
             dimnames = 
               list(c(), c("mu1", "mu2", "eta","sigma.sq","tau.sq"))
             )
```



# Summary and Short Discussion

Put convergence problem aside, one can see JAGS results were similar to MH, and independence MH is similar to AR.


From the results above one can notice that if the posterior mean of $\mu_1$, $\mu_2$ closer to sample mean (and with smaller empirical s.d.), smaller the $\sigma^2$ is and thus more deviate from its sample estimate. It seems to be a tradeoff.




